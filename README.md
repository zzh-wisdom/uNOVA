# uNOVA

user space NOVA file system

## 设计

一种具有细粒度数据管理的持久内存文件系统
A persistent memory file system with fine-grained data management

FineFS(file system with **fine**-grained data management)

创新点：

1. 设计混合粒度的空间管理机制，大数据(>=4kb)采用page粒度，小数据（<4kb）采用cacheline粒度，使得文件系统在小数据更新时也具有优越的性能
2. 设计一种新颖的log和journal技术，在保证崩溃一致性的同时，最大限度地减少flush和fence的个数。
3. 每个线程log和journal技术（待定）
4. 针对optance memory的特性进行优化，多线程扩展性、同一个cacheline的重复flush、ntstore，顺序写(待定)

slab尽量同一个page分配（顺序）。（测试不同page大小时的性能）

总之，本文我们对aep进行全面细致的分析，并设计尽可能适应aep特性、并且具有良好线程扩展性的读写策略，以最大限度发挥aep的性能。

NOVA [54， 55] 是一个日志结构的 NVMM 文档系统，它为每个文档和目录维护一个单独的日志，并使用写入时复制进行文档数据更新以确保数据一致性。最初的NOVA研究使用模拟的NVMM进行评估，因此NOVA尚未针对Optane进行调整。(来自aep调研论文)

最初的NOVA设计有两个特性会降低傲腾的性能。首先，NOVA 为每个元数据更新附加的日志条目很小 – 40-64 B，并且由于 NOVA 使用许多日志，日志更新几乎没有局部性，尤其是当文档系统处于load负载时。其次，NOVA 使用写入时复制到 4 kB 页面进行文档数据更新，导致无用的存储。无论底层内存技术如何，都会发生这种低效率，但 Optane 糟糕的store性能加剧了其影响。

我们通过增加**日志条目的大小**并避免一些写入时复制操作来解决这两个问题。我们的 NOVA 修改版本（NOVA-datalog ）将 sub-page 写入的数据嵌入到日志中（图 10）。与日志的普通写入条目（包含指向新的写入时复制 4 kB 页及其在文档中的偏移量）不同，嵌入写入条目包含页面偏移量、页面内的地址，后跟写入的实际内容。这种优化需要对原始NOVA设计进行一些辅助更改。**特别是，NOVA 必须在内存映射或读取文档之前将子页面更新合并到目标页面中**。

> 所以它的都延迟会比较高。而且好像没有提到使用每个线程log的思想

为了提高读性能，加入碎片整理机制，小于某个阈值，比如256B，在读的时候直接整理到原页面（或者采用页面缓存，感觉这个方法可行，而且还能缓解不对齐写造成的性能下降）。大于阈值的，则直接读（可选放页面缓存）

页面缓存可以极大的缓解Optance小的随机读取问题。

> 或许可以这样选择，小于256B的直接写log，大于256B的slab分配器，看哪种效果好。（原因是，随机ntstore时256B已经接近带宽的峰值）

最初的 NOVA 设计没有试图限制每个 DIMM 的写入器数量。事实上，它倾向于从连续区域为文档分配页面，通过交错，倾向于将这些页面分布在 DIMM 上。为**了解决此问题，我们将计算机配置为将写入器线程固定到非交错的傲腾 DIMM**。此配置可确保线程和 NVDIMM 之间的均匀匹配，从而平衡负载并最大化每个 NVDIMM 的带宽。

> 采用非交错的方式

### 小写问题

按照64B为粒度进行管理，写入时，小于64B的写，直接copy-on-write。大于64B的，类似nova，分配64B整数倍的连续空间，进行写入。

对于64B连续空间分配问题，有两种方案：slab分配器 / log-structure的方式。

前者不需要垃圾回收，更加稳定。后者需要垃圾回收，但能发挥顺序写的性能（但测试时发现，顺序和随机的ntstore带宽是差别不大的，具体可以继续测试）。因此先选择前者

### 多个log造成的随机写问题

改成每个cpu一个单独的log。
带来的问题是，不同文件的log混合在一起，恢复时间比较久，不能做到只恢复某个文件，为了缓解这个问题，正常关闭时，每个inode都有一个单独的log用来记录所有属于它的log entry的位置。由于我们只用记录log entry的偏移，因此数据量非常少。恢复时，根据log信息，就可以扫描特定位置的log entry进行恢复。

另外考虑到目录和文件的属性不同，io模式也不一样，将它们的操作混合在同一个log中会增加管理成本，参考f2fs的思想，我们将file和dir的操作分别记录到不同的log中。其中dentry log是比较大的，采用类似kv分离的思想，小文件名内嵌到log中，大文件名则额外分配slab/page进行存放，然后log entry中有个指针指向它。

同理，file和dir也分别用不同的slab分配器进行数据隔离。相比nova，我们的方案支持更长的文件名。最长支持4KB，但如果大于4KB其实也可以支持，需要在每个page后面添加一个指针。

因此对于一个具有32core的机器，共需要32x2个slab分配器和32x2个log。

### 对于索引

尽量做到只用一次NVM访问，内存需要保留更多的元信息。

### 精心设计的log 数据一致性

靠log/journal，一个高效的log机制非常重要。由于我们只讲元数据部分进行log，数据量是比较小的，因此我们的目标非常明确，就是尽可能地提高小粒度entry的log性能。而元数据的崩溃一致性通常是通过log和journal机制来实现的，因此设计一个性能优越的log机制，对于文件系统的元数据操作影响更大（至关重要）。

大于等于256B用ntstore。小于256B时尽量顺序，且用普通的store

## 支持的操作

- mkdir
- rmdir
- open
- unlink

- write
- read
- ftruncate


## TODO

1. 测试不对齐写，参考这里的参数 <file:///C:/Users/Mrzho/Desktop/github/%E7%A1%95%E5%A3%AB%E6%AF%95%E8%AE%BE/practice-ssd/fio/html-doc/fio_man.html#i-o-type>
2. 解决多线程扩展性问题，考虑公平信号量。nt cpy时进行互斥
3. 看不同的io engine和io type，应该有其他负载

